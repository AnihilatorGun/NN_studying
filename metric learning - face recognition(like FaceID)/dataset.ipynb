{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1k9J2sN8BR6PUZaKrMlMgGB_ZZIJPk-5H","authorship_tag":"ABX9TyPbQB5ccwUmGSW/flmv0m9k"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"smy1IHEJijF6","executionInfo":{"status":"ok","timestamp":1675941274815,"user_tz":-180,"elapsed":9051,"user":{"displayName":"Nikita Ushakov","userId":"10694674250451986039"}}},"outputs":[],"source":["import numpy as np  # Load required libs\n","import pandas as pd\n","import torch\n","import torchvision\n","from typing import Tuple, List, Type, Dict, Any\n","from sklearn.utils import shuffle\n","from torch.autograd import Variable\n","from threading import Thread, Lock\n","import os\n","import pickle\n","import datetime\n","import gzip"]},{"cell_type":"code","source":["class Threadsafe_iter:\n","    \"\"\"\n","    Takes an iterator/generator and makes it thread-safe by\n","    serializing call to the `next` method of given iterator/generator.\n","    You have to use it in a such way - Threadsafe_iter(get_objects_i(len(#your_unsafe_list)))\n","    \"\"\"\n","    def __init__(self, it):\n","        self.it = it\n","        self.lock = Lock()\n","\n","    def __iter__(self):\n","        return self\n","\n","    def __next__(self):\n","        with self.lock:\n","            return next(self.it)\n","\n","def get_objects_i(objects_count):\n","    \"\"\"Cyclic generator of paths indices\"\"\"\n","    current_objects_id = 0\n","    while True:\n","        yield current_objects_id\n","        current_objects_id  = (current_objects_id + 1) % objects_count"],"metadata":{"id":"e87f42WCircg","executionInfo":{"status":"ok","timestamp":1675942447402,"user_tz":-180,"elapsed":2,"user":{"displayName":"Nikita Ushakov","userId":"10694674250451986039"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["class Cifar_dataset:\n","    def __init__(self,\n","                 batch_size: int,\n","                 path_to_batches: str,\n","                 transform_cpu = None,\n","                 train : bool = True):\n","        self.batch_size = batch_size\n","        self.train = train\n","        self.transform_cpu = transform_cpu\n","        self.path_to_batches = path_to_batches\n","\n","        self._read_data()\n","\n","        self.new_epoch = True\n","        self.yield_lock = Lock()\n","        self.lock = Lock()\n","        self.safe_iterator = Threadsafe_iter(get_objects_i(len(self.indexes)))\n","        self.shuffle()\n","\n","    def unpickle(file):\n","        import pickle\n","        with open(file, 'rb') as fo:\n","            d = pickle.load(fo, encoding='latin1')\n","        return d\n","\n","    def shuffle(self):\n","        self.indexes = shuffle(self.indexes)\n","\n","    def __len__(self):\n","        return len(self.indexes)\n","\n","    def get_batch_size(self):\n","        return self.batch_size\n","\n","    def get_steps_per_epoch(self):\n","        return len(self) // self.get_batch_size() + 1\n","\n","    def __iter__(self):\n","        \"\"\"\n","        TODO!\n","        \"\"\" \n","        while True:\n","            with self.lock:\n","                if self.new_epoch:\n","                    self.new_epoch = False\n","                    self.shuffle()\n","                    self.batch = []\n","            for ID_gen in self.safe_iterator:  # Solve thread sequence problem, making each thread await for the previous one\n","                img = self.data[ID_gen].reshape((3, 32, 32))\n","                img = torch.from_numpy(img)\n","                target = torch.tensor(self.target[ID_gen])\n","                if self.transform_cpu:\n","                    img = self.transform_cpu(img)\n","                data = {'img':img, 'target':target}                                    \n","                with self.yield_lock:  # Solve thread sequence problem making each tread await for the previous one\n","                    if len(self.batch) < self.batch_size:\n","                        self.batch.append(data)\n","                    if len(self.batch) % self.batch_size == 0:                   \n","                        yield self.batch\n","                        self.batch = []\n","            with self.lock:\n","                self.new_epoch = True\n","                self.shuffle()\n","\n","    def _read_data(self):\n","        if self.train:\n","            if self.path_to_batches == '':\n","                state_filenames = list('data_batch_'+str(i+1) for i in range(5))\n","            else:\n","                state_filenames = list(self.path_to_batches + '/'+'data_batch_'+str(i+1) for i in range(5))\n","        else:\n","            if self.path_to_batches == '':\n","                state_filenames = ['test_batch', ]\n","            else:\n","                state_filenames = [self.path_to_batches + '/' + 'test_batch']\n","\n","        self.data = None\n","        self.target = None\n","        for filename in state_filenames:\n","            if self.data is None and self.target is None:\n","                data_dict = Cifar_dataset.unpickle(filename)\n","                self.target = data_dict['labels']\n","                self.data = data_dict['data']\n","            else:\n","                data_dict = Cifar_dataset.unpickle(filename)\n","                self.target = np.concatenate((self.target, data_dict['labels']))\n","                self.data = np.concatenate((self.data, data_dict['data']))\n","\n","        self.indexes = np.arange(0, len(self.data))"],"metadata":{"id":"b0crHv2XxNxY","executionInfo":{"status":"ok","timestamp":1675942797385,"user_tz":-180,"elapsed":2,"user":{"displayName":"Nikita Ushakov","userId":"10694674250451986039"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["class Dataset:\n","    def __init__(self,\n","                 batch_size : int,\n","                 pathes_list,\n","                 ID_to_number,\n","                 train : bool = True,\n","                 transform_cpu = None):\n","        \n","        self.pathes_list = pathes_list\n","        self.ID_to_number = ID_to_number\n","        self.batch_size = batch_size\n","        self.transform_cpu = transform_cpu\n","        self.train = train\n","\n","        self._create_correspondence()\n","\n","        self.new_epoch = True\n","        self.yield_lock = Lock()\n","        self.lock = Lock()\n","        self.safe_iterator = Threadsafe_iter(get_objects_i(len(self.pathes_list)))\n","        self.shuffle()\n","\n","    def shuffle(self):\n","        self.pathes_list = shuffle(self.pathes_list)\n","    \n","    def __len__(self):\n","        return len(self.pathes_list)\n","\n","    def get_batch_size(self):\n","        return self.batch_size\n","\n","    def get_steps_per_epoch(self):\n","        return len(self) // self.get_batch_size() + 1\n","    \n","    def __iter__(self):\n","        \"\"\"\n","        TODO!\n","        \"\"\" \n","        while True:\n","            with self.lock:\n","                if self.new_epoch:\n","                    self.new_epoch = False\n","                    self.shuffle()\n","                    self.batch = []\n","            for ID_gen in self.safe_iterator:  # Solve thread sequence problem, making each thread await for the previous one\n","                if self.train:\n","                    path1 = self.pathes_list[ID_gen]\n","                    ID = path1.split('_')[-2].split('/')[-1]\n","\n","                    path2 = path1\n","                    ID_list = self.ID_path_correspondence[ID]\n","                    while path2 == path1:\n","                        path2 = ID_list[np.random.randint(len(ID_list))]\n","                    \n","                    anchor = torchvision.io.read_image(path1).float()\n","                    positive = torchvision.io.read_image(path2).float()\n","                    ID = torch.tensor(self.ID_to_number[ID])\n","\n","                    if self.transform_cpu:\n","                        anchor = self.transform_cpu(anchor)\n","                        positive = self.transform_cpu(positive)\n","                    data = {'anchor':anchor, 'positive':positive, 'ID':ID}\n","                else:\n","                    path1 = self.pathes_list[ID_gen]\n","                    ID = path1.split('_')[-2].split('/')[-1]\n","                    \n","                    anchor = torchvision.io.read_image(path1).float()\n","                    ID = torch.tensor(self.ID_to_number[ID])\n","\n","                    if self.transform_cpu:\n","                        anchor = self.transform_cpu(anchor)\n","                    data = {'anchor':anchor, 'ID':ID}\n","\n","                                    \n","                with self.yield_lock:  # Solve thread sequence problem making each tread await for the previous one\n","                    if len(self.batch) < self.batch_size:\n","                        self.batch.append(data)\n","                    if len(self.batch) % self.batch_size == 0:                   \n","                        yield self.batch\n","                        self.batch = []\n","            with self.lock:\n","                self.new_epoch = True\n","                self.shuffle()\n","\n","    def get_batch_size(self):\n","        return self.batch_size\n","\n","    def get_steps_per_epoch(self):\n","        return len(self) // self.batch_size + 1\n","\n","    def _create_correspondence (self):\n","        \"\"\"\n","        This method creates two containers:\n","\n","        *ID_path_correspondence - takes ID and return list of pathes\n","        ID <-> Person, path <-> concrete face\n","\n","        *ID_to_number - takes ID and returns its 'number' (ID's aren't\n","        distrubuted like 0,1,2...)\n","        \"\"\"\n","        self.ID_path_correspondence = {}\n","        for file_path in self.pathes_list:\n","            file_id = file_path.split('_')[-2].split('/')[-1]\n","\n","            if file_id in self.ID_path_correspondence:\n","                self.ID_path_correspondence[file_id].append(file_path)\n","            else:\n","                self.ID_path_correspondence[file_id] = [file_path, ]\n","\n","    def make_train_val_split(batch_size : int,\n","                             ratio : int = 0.8,\n","                             path : str = '',\n","                             cpu_transform_train = None,\n","                             cpu_transform_val = None,\n","                             least_size = 10,\n","                             seed = 42):\n","        if path == '':\n","            files_path = 'CASIA-WebFace_crop/'\n","        else:\n","            files_path = path + '/' + 'CASIA-WebFace_crop/'\n","\n","        filenames_list = os.listdir(files_path)\n","        ID_path_correspondence = {}\n","        for filename in filenames_list:\n","            ID = filename.split('_')[-2]\n","            file_path = files_path + filename\n","            if ID in ID_path_correspondence:\n","                ID_path_correspondence[ID].append(file_path)\n","            else:\n","                ID_path_correspondence[ID] = [file_path, ]\n","\n","        train_file_pathes = []\n","        val_file_pathes = []\n","        IDs_to_delete = []\n","\n","        np.random.seed(seed)\n","\n","        for ID in ID_path_correspondence:\n","            if len(ID_path_correspondence[ID]) >= least_size:\n","                mask = np.random.rand(len(ID_path_correspondence[ID])) < ratio\n","                for idx, path in enumerate(ID_path_correspondence[ID]):\n","                    if mask[idx]:\n","                        train_file_pathes.append(path)\n","                    else:\n","                        val_file_pathes.append(path)\n","            else:\n","                IDs_to_delete.append(ID)\n","        for ID in IDs_to_delete:\n","            ID_path_correspondence.pop(ID, None)\n","\n","        number = 0\n","        ID_to_number = {}\n","        for ID in ID_path_correspondence:\n","            ID_to_number[ID] = number\n","            number += 1\n","\n","        return (Dataset(batch_size,\n","                       train_file_pathes,\n","                       ID_to_number,\n","                       train=True,\n","                       transform_cpu = cpu_transform_train),\n","                Dataset(batch_size,\n","                        val_file_pathes,\n","                        ID_to_number,\n","                        train=False,\n","                        transform_cpu = cpu_transform_val))"],"metadata":{"id":"ZCByrNoCiyTl"},"execution_count":null,"outputs":[]}]}