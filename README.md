# Hi! This repo represents my way of studying DeepLearning
[My LinkedIn:)](https://www.linkedin.com/in/nikita-ushakov-b62725272/)
____
## Resources that I use:
+ [Technotrack 2021](https://github.com/mailcourses/technotrack-NN2021S-lectures)
+ [HSE DeepLearning](https://github.com/hse-ds/iad-deep-learning/tree/master/2022)
+ [MIPT DeepLearningSchool](https://github.com/DLSchool/deep-learning-school)
+ [Tinkoff course](https://algocode.ru/dlfall22/)
+ [Dive into Deep Learning](http://d2l.ai/index.html)
+ [Other guides](https://github.com/ahmedbahaaeldin/From-0-to-Research-Scientist-resources-guide)
____
## Plans:
- [ ] Finish technotrack's lectures
- [ ] Make DeTR-like decoder for BERT encoder (multilable classification task)
- [x] Understand NVIDIA-Dali (not at all, mix of shader-code and python makes it's work unstable and confusing)
- [x] Look throught the Tinkoff's lectures
- [x] Refactor cloudiness estimation project
- [x] Learn how does Transformers work
- [x] Learn how does GAN work
- [x] CV trick (Squeeze&Excitation, DepthwiseSeparateConv and so on)
- [x] Autograd tricks
----
## Building blocks:
- [ ] Knoledge distillation, pruning
- [x] Transformers
- [x] GAN
- [x] Autoencoders
- [x] Metric learning
____
## Pet projects:
- [ ] BERToDeTR for multilable classification (BERT as backbone)
- [ ] Cloudiness estimation (almost done, some minimal tests left)
- [ ] FaceGeneration (there are some basic results)
- [ ] DeTR for detection and segmentation
- [x] Judging trampoline acrobatics (commertial one, there is video as a proof)
- [x] Face recognition
____
## Technologies used:
- PyTorch-Lightning
- Wandb
- Connectome
- NVIDIA-Dale
- Composer
- Torch-Pruning (a bit familiar)
- transformers, datasets (Huggingface)