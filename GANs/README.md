# GANs
____
## Here one can find my realization of gans. GAN consists of two main part - Discriminator(D) and Generator(G). First one tries to distinguish original data(say, images from some dataset) from the images generated by G. There train independently, D makes first step, then G makes step and so on. I have created modificated version, called VAE/WGAN-GP, it's description is given below.

____
VAE/WGAN GP consists of three parts - Encoder(E), Generator-decoder(G) and Discriminator(D). The idea is to make G map random noise to data which is distributed as the data in the dataset, but also make E and G work as a couple - E maps original data to the embedding space and G tries to restore the original data. That helps G not to produce adversarial attacks, as it can be in the usual GANs, but still has detailed data as it is in the GANs. 
![VAE-WGAN Architecture](https://github.com/AnihilatorGun/NN_studying/blob/master/GANs/images/vae_gan.png)
![Baseline generation based on 20% of the CelebA](https://github.com/AnihilatorGun/NN_studying/blob/master/GANs/images/celeba_generated.png)

____
## TODO:
- [ ] Tune hyperparameters
- [ ] AAE part to replace the KL in the E-G part
- [ ] Good results with CelebA
- [x] Modifications with MNIST
- [x] Basic results with CelebA
- [x] Basic results with MNIST
- [x] VAE/WGAN-GP oprimization using PyTorch-lightning
- [x] Generator with base block and bottleneck block
- [x] All the losses in the one class
- [x] Encoder, Discriminator with base blocks
____
## Hypotheses:
- [ ] PReLU makes G training unstable
- [ ] Pretrained model as Featrue Extractor for MSE in the E-G training makes whole training more stable and improves results
- [x] 2 steps for D and 1 step for E-G is better, than they all step only once (Seems, when D a bit saturetes provides more information for G, also GP helps not to collapse the gradient)
- [x] Bias=True in the first G's layer (FC) improves the risk of mode collapse (Tested on MNIST, when G is quite more complicated, than D, it generates constant image. I think the reason is bias because it becomes harder to train G to make non-trivial data and it collapses)
- [x] When MSE in the E-G training part has only null block (when original data compares with generated data only, without D processing) too high KL-coefficient makes mode collapse (Tested on MNIST, when KL-coefficient grows mode collapse is occured. I think that this happens because it becomes to complicated to make generated mu != 0 and logvar != 1 so G can't produce nothing but constant image to make MSE low)
- [x] SGD in the G still works more stable as the AdaM with betas=(0.2, 0.999) (Not sure, but may be AdaM because of moments can make G saturate and produce adversial attacks on D)
- [x] If MSE in the E-G part contains D layers generated images have artifacts (Tested on MNIST and have some visual results on CelebA. One can see on the generated images in the section "CelebA baseline" artefacts that look like a waves on the water. Besides they have similar structure and they are global, so they appear in the deep layers. As they have similar structure, it could be connected with adversial attacks on D, but only MSE in the E-G training provides information from D's deep layers so it is. When D's feature extractor is removed from MSE in the E-G training, artifacts don't appear in MNIST experiment)
- [x] Bottleneck layers must be put with respect to feature spatial size, for example if the spatial size is 2^n one must put 2^(n-2) (Say, after FC all the pixels are correlated with each other, after upsample we have w=h=4, and we must correlate other pixels. It could be done with one bottleneck block while it has conv3x3, and finally decreases the number of channels by 2 times. That creates new details on the image, and after upsample details must be correlated with each other, so one could make perceptive field bigger in two times)