{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPhxWxbl9RT26MbbFIdzAgG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"uBKDSTZvhs0S"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"0sGljOxen6WT"},"outputs":[],"source":["import random\n","import torchvision\n","import torch\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import time\n","from tqdm.notebook import tqdm\n","import os\n","import zipfile\n","from PIL import Image\n","from collections import defaultdict\n","import copy\n","from torch.nn import ReLU, Conv2d, BatchNorm2d, Sequential, AdaptiveAvgPool2d, Linear, MaxPool2d, Flatten, CrossEntropyLoss, PReLU, InstanceNorm2d, LeakyReLU, AvgPool2d\n","try:\n","    import pytorch_lightning as pl\n","except:\n","    !pip install pytorch-lightning\n","    import pytorch_lightning as pl\n","\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # We'd like to use GPU\n","device"]},{"cell_type":"code","source":["class CelebADataset:\n","    \"\"\"\n","    Map-style dataset of celeb-faces, contain about 200k photoes of about 10k \n","    persons.\n","    --Class method make_train_test_dataset returns train and test datasets with\n","    adequate class-split structure (say, if there are 10 photoes of person A\n","    will train split contain 8 photoes, test split will contain remaining\n","    photoes, i.e. splitting works for classes)\n","    Args:\n","    -zip_path - path to zipfile which contains photoes\n","    -annotation_path - path to annotation file\n","    -extraction_path - path where photoes will be extracted\n","    -transform_train - transformation which will be applied to train images\n","    -transform_test - transformation which will be applied to test images\n","    -train_test_ration - multiplied by 100, this number shows the percentage of\n","    images will be placed in train dataset(other imgs will be placed to \n","    test_dataset)\n","    -seed - random seed, requires for reproducibility and ensembles\n","    -min_num_imgs_in_class - minimum number of classes when \n","    \"\"\"\n","    def __init__(self, pathes_list, transform=None, if_cache=False):\n","        self.__pathes_list = pathes_list\n","        self.__transform = transform\n","\n","        self.if_cache = if_cache\n","        if if_cache:\n","            self.img_list = []\n","            with tqdm(total=len(self.__pathes_list)) as pbar:\n","                for path in pathes_list:\n","                    pbar.update()\n","                    self.img_list.append(torchvision.io.read_image(path, mode = torchvision.io.ImageReadMode.RGB))\n","\n","    def __len__(self):\n","        return len(self.__pathes_list)\n","\n","    def __getitem__(self, idx):\n","        if not self.if_cache:\n","            path = self.__pathes_list[idx]\n","            img = torchvision.io.read_image(path, mode = torchvision.io.ImageReadMode.RGB)\n","            if self.__transform:\n","                img = self.__transform(img)\n","            return [img, ]\n","        else:\n","            img = self.img_list[idx]\n","            if self.__transform:\n","                img = self.__transform(img)\n","            return [img, ]\n","\n","    def make_dataset(zip_path=\"drive/MyDrive/GitHub/NN studying/NN_studying/face recognition/data/img_align_celeba.zip\",\n","                     annotation_path = \"drive/MyDrive/GitHub/NN studying/NN_studying/face recognition/data/identity_CelebA.txt\",\n","                     extract_path=\"\",\n","                     transform=None,\n","                     ratio=0.15,\n","                     seed=1337,\n","                     if_cache=False):\n","        if not extract_path == \"\":\n","            if extract_path[-1] != \"/\":\n","                extract_path += \"/\"\n","\n","        if not os.path.exists(extract_path):\n","            data_zip = zipfile.ZipFile(zip_path)\n","            data_zip.extractall(extract_path)\n","            data_zip.close() \n","        \n","        extract_path += \"img_align_celeba/\"\n","\n","        pathes_list = []\n","        with open(annotation_path) as f:\n","            for line in f:\n","                path, id = line.split('\\n')[0].split(' ')\n","                id = int(id)\n","                pathes_list.append(extract_path+path)\n","\n","        np.random.seed(seed)\n","        pathes_list = np.random.choice(pathes_list, size=int(ratio*len(pathes_list)), replace=False)\n","\n","        return CelebADataset(pathes_list, transform, if_cache)"],"metadata":{"id":"74OHbuTHJxkL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_dtype(dtype_str):\n","    if dtype_str == 'fl32':\n","        return torch.float\n","    elif dtype_str == 'fl16':\n","        return torch.half\n","\n","class LinearHe(torch.nn.Module):\n","    def __init__(self, in_f, out_f, bias=True, dtype='fl32'):\n","        super(LinearHe, self).__init__()\n","        dtype = get_dtype(dtype)\n","\n","        he_weight = torch.tensor(torch.sqrt(2 / in_f), dtype=dtype)\n","        self.weight = torch.nn.Parameter(torch.randn(size=(out_f, in_f), dtype=dtype) * he_weight)\n","        self.bias = torch.nn.Parameter(torch.zeros(size=(out_f, ), dtype=dtype)) if bias else None\n","\n","    def forwaard(self, x):\n","        return torch.nn.functional.linear(x, self.weight, self.bias)\n","\n","class Conv2dHe(torch.nn.Module):\n","    def __init__(self, in_c, out_c, kernel_size=3, padding=0, stride=1, bias=False, dtype='fl32'):\n","        super(Conv2dHe, self).__init__()\n","        dtype = get_dtype(dtype)\n","        self.padding = padding\n","        self.stride = stride\n","\n","        he_weight = torch.tensor(torch.sqrt(2 / (in_c * (kernel_size ** 2))), dtype=dtype)\n","        self.weight = torch.nn.Parameter(torch.randn(size=(out_c, in_c, kernel_size, kernel_size), dtype=dtype))\n","        self.bias = torch.nn.Parameter(torch.zeros(size=(out_c, ), dtype=dtype)) if bias else None\n","\n","    def forward(self, x):\n","        return torch.nn.functional.conv2d(x, self.weight, self.bias, padding=self.padding, stride=self.stride)\n","\n","class AdaIN(torch.nn.Module):\n","    def __init__(self, eps=1e-8):\n","        super(AdaIN, self).__init__()\n","        self.eps = eps\n","\n","    def forward(self, x, y_s, y_i):\n","        x_mean, x_std = torch.mean(x, dim=(2, 3), keepdim=True), torch.std(x, dim=(2, 3), keepdim=True)\n","        x = (x - x_mean) / (x_std + eps)\n","        return (x * y_s + y_i)"],"metadata":{"id":"XutN0WTpl6ZQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"IIQf11_QI5Nc"},"execution_count":null,"outputs":[]}]}