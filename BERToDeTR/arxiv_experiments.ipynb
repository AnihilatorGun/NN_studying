{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport datasets\nfrom transformers import AutoTokenizer, AutoModel\nimport torch\nimport torch.nn.functional as F\nfrom torch import nn, Tensor\nfrom tqdm.notebook import tqdm\nimport datasets\nfrom scipy.optimize import linear_sum_assignment\ntry:\n    import composer.functional as cf\nexcept:\n    !pip install mosaicml\n    import composer.functional as cf\ntry:\n    import transformers\nexcept:\n    !pip install -U adapter-transformers  > /dev/null\n    import transformers\ntry:\n    import pytorch_lightning as pl\nexcept:\n    !pip install pytorch-lightning\n    import pytorch_lightning as pl\ntry:\n    import adabelief_pytorch\nexcept:\n    !pip install adabelief_pytorch==0.2.0\n    import adabelief_pytorch\nfrom transformers import BertTokenizer, DistilBertTokenizerFast, BertForSequenceClassification\nfrom sklearn.metrics import f1_score, roc_auc_score, accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import DataLoader\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:27:39.843387Z","iopub.execute_input":"2023-06-09T12:27:39.843729Z","iopub.status.idle":"2023-06-09T12:28:23.943078Z","shell.execute_reply.started":"2023-06-09T12:27:39.843699Z","shell.execute_reply":"2023-06-09T12:28:23.941871Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Collecting mosaicml\n  Downloading mosaicml-0.14.1-py3-none-any.whl (565 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m565.9/565.9 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hCollecting pyyaml<7,>=6.0 (from mosaicml)\n  Downloading PyYAML-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (682 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m682.2/682.2 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm<5,>=4.62.3 in /opt/conda/lib/python3.10/site-packages (from mosaicml) (4.64.1)\nCollecting torchmetrics<0.11.4,>=0.10.0 (from mosaicml)\n  Downloading torchmetrics-0.11.3-py3-none-any.whl (518 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m518.6/518.6 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting torch-optimizer<0.4,>=0.3.0 (from mosaicml)\n  Downloading torch_optimizer-0.3.0-py3-none-any.whl (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.9/61.9 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: torchvision<0.16,>=0.11.0 in /opt/conda/lib/python3.10/site-packages (from mosaicml) (0.15.1)\nRequirement already satisfied: torch<2.1,>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from mosaicml) (2.0.0)\nRequirement already satisfied: requests<3,>=2.26.0 in /opt/conda/lib/python3.10/site-packages (from mosaicml) (2.28.2)\nRequirement already satisfied: numpy<1.25.0,>=1.21.5 in /opt/conda/lib/python3.10/site-packages (from mosaicml) (1.23.5)\nRequirement already satisfied: psutil<6,>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from mosaicml) (5.9.3)\nCollecting coolname<3,>=1.1.0 (from mosaicml)\n  Downloading coolname-2.2.0-py2.py3-none-any.whl (37 kB)\nRequirement already satisfied: tabulate==0.9.0 in /opt/conda/lib/python3.10/site-packages (from mosaicml) (0.9.0)\nRequirement already satisfied: py-cpuinfo<10,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from mosaicml) (9.0.0)\nRequirement already satisfied: packaging<23,>=21.3.0 in /opt/conda/lib/python3.10/site-packages (from mosaicml) (21.3)\nRequirement already satisfied: importlib-metadata<7,>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from mosaicml) (5.2.0)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.10/site-packages (from importlib-metadata<7,>=5.0.0->mosaicml) (3.15.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging<23,>=21.3.0->mosaicml) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.26.0->mosaicml) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.26.0->mosaicml) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.26.0->mosaicml) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.26.0->mosaicml) (2023.5.7)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch<2.1,>=1.10.0->mosaicml) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch<2.1,>=1.10.0->mosaicml) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch<2.1,>=1.10.0->mosaicml) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch<2.1,>=1.10.0->mosaicml) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch<2.1,>=1.10.0->mosaicml) (3.1.2)\nCollecting pytorch-ranger>=0.1.1 (from torch-optimizer<0.4,>=0.3.0->mosaicml)\n  Downloading pytorch_ranger-0.1.1-py3-none-any.whl (14 kB)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision<0.16,>=0.11.0->mosaicml) (9.5.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch<2.1,>=1.10.0->mosaicml) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch<2.1,>=1.10.0->mosaicml) (1.3.0)\nInstalling collected packages: coolname, pyyaml, torchmetrics, pytorch-ranger, torch-optimizer, mosaicml\n  Attempting uninstall: pyyaml\n    Found existing installation: PyYAML 5.4.1\n    Uninstalling PyYAML-5.4.1:\n      Successfully uninstalled PyYAML-5.4.1\n  Attempting uninstall: torchmetrics\n    Found existing installation: torchmetrics 0.11.4\n    Uninstalling torchmetrics-0.11.4:\n      Successfully uninstalled torchmetrics-0.11.4\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndask-cudf 23.4.1 requires cupy-cuda11x<12.0.0a0,>=9.5.0, which is not installed.\ndask-cuda 23.4.0 requires dask==2023.3.2, but you have dask 2023.5.0 which is incompatible.\ndask-cudf 23.4.1 requires dask==2023.3.2, but you have dask 2023.5.0 which is incompatible.\ndistributed 2023.3.2.1 requires dask==2023.3.2, but you have dask 2023.5.0 which is incompatible.\njupyterlab-lsp 4.1.0 requires jupyter-lsp>=2.0.0, but you have jupyter-lsp 1.5.1 which is incompatible.\nkfp 1.8.21 requires google-api-python-client<2,>=1.7.8, but you have google-api-python-client 2.86.0 which is incompatible.\nkfp 1.8.21 requires PyYAML<6,>=5.3, but you have pyyaml 6.0 which is incompatible.\nraft-dask 23.4.1 requires dask==2023.3.2, but you have dask 2023.5.0 which is incompatible.\nydata-profiling 4.1.2 requires scipy<1.10,>=1.4.1, but you have scipy 1.10.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed coolname-2.2.0 mosaicml-0.14.1 pytorch-ranger-0.1.1 pyyaml-6.0 torch-optimizer-0.3.0 torchmetrics-0.11.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"},{"name":"stdout","text":"Collecting adabelief_pytorch==0.2.0\n  Downloading adabelief_pytorch-0.2.0-py3-none-any.whl (5.7 kB)\nRequirement already satisfied: torch>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from adabelief_pytorch==0.2.0) (2.0.0)\nRequirement already satisfied: colorama>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from adabelief_pytorch==0.2.0) (0.4.6)\nRequirement already satisfied: tabulate>=0.7 in /opt/conda/lib/python3.10/site-packages (from adabelief_pytorch==0.2.0) (0.9.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->adabelief_pytorch==0.2.0) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->adabelief_pytorch==0.2.0) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->adabelief_pytorch==0.2.0) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->adabelief_pytorch==0.2.0) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=0.4.0->adabelief_pytorch==0.2.0) (3.1.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=0.4.0->adabelief_pytorch==0.2.0) (2.1.2)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=0.4.0->adabelief_pytorch==0.2.0) (1.3.0)\nInstalling collected packages: adabelief_pytorch\nSuccessfully installed adabelief_pytorch-0.2.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"research multilable\")\nwandb.login(key=secret_value_0)","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:28:23.945653Z","iopub.execute_input":"2023-06-09T12:28:23.946017Z","iopub.status.idle":"2023-06-09T12:28:28.364113Z","shell.execute_reply.started":"2023-06-09T12:28:23.945959Z","shell.execute_reply":"2023-06-09T12:28:28.363118Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"def prepare_data():\n    df = pd.read_json('/kaggle/input/arxiv/arxiv-metadata-oai-snapshot.json', lines=True)\n    df = df.drop(columns=['id', 'submitter', 'authors', 'comments', 'journal-ref', 'doi', 'report-no', 'license', 'versions', 'update_date', 'authors_parsed'])\n    df['categories'] = df['categories'].apply(lambda i: i.split(' '))\n    category_counts = df.explode('categories')['categories'].value_counts()\n    print('Start category clearing... ')\n\n    def clear_categories(category_counts, threshold=20000):\n        remain_categories = category_counts > threshold\n        def wrapped(category_list):\n            temp = remain_categories[category_list]\n            return list(temp[temp == True].index)\n        return wrapped\n\n    df['categories'] = df['categories'].apply(clear_categories(category_counts))\n    print('Finished')\n    df['category_number'] = df['categories'].apply(len)\n    df = df[df['category_number'] != 0]\n    category_counts = df.explode('categories')['categories'].value_counts()\n    print(category_counts)\n    print(len(df))\n\n    from sklearn.preprocessing import MultiLabelBinarizer\n    df['text'] = df['title'] + '. ' + df['abstract']\n    df = df.drop(columns=['title', 'abstract'])\n\n    def preprocessing(data):\n        data['text'] = [a.strip() for a in data['text']]\n        data['text'] = data['text'].str.replace('\\n', ' ', regex=False).str.replace('\\t', ' ', regex=False).str.replace(r'\\s\\s+', ' ', regex = True)\n        return data\n\n    df = preprocessing(df)\n\n    # Extract the categories column as a list of lists\n    categories = []\n    for el in df[\"categories\"]:\n        categories.extend(el)\n    categories = np.unique(categories)\n    NUM_LABELS = len(categories)\n\n    # Initialize the MultiLabelBinarizer and fit_transform the categories\n    mlb = MultiLabelBinarizer()\n    df['labels'] =  mlb.fit_transform(df[\"categories\"].values).tolist()\n\n    df_train = df.sample(frac=0.8, random_state=42)\n    df_test = df.drop(df_train.index)\n    del(df)\n\n    def there_is_category(category):\n        def wrapped(category_list):\n            for cat in category_list:\n                if cat == category:\n                    return True\n            return False\n        return wrapped\n\n    def normalize_categories(df, num_sample=20000):\n        category_count = df.explode('categories')['categories'].value_counts()\n        df['taken'] = False\n        df['there_is_category'] = False\n        categorized_dfs = []\n        with tqdm(total=len(category_count)) as pbar:\n            for category in reversed(list(category_count.index)):\n                pbar.update()\n                df['there_is_category'] = df['categories'].apply(there_is_category(category))\n                suit_df = df[(df['there_is_category'] == True) & (df['taken'] == False)]\n                if len(suit_df) >= num_sample:\n                    df_category = suit_df.sample(n=num_sample)\n                elif len(suit_df) >  0:\n                    # this algorith takes whole remainig rows at least once(!)\n                    temp_df_list = []\n                    temp_df_list.append(suit_df)\n                    temp_df_list.append(suit_df.sample(n=num_sample-len(suit_df), replace=True))\n                    df_category = pd.concat(temp_df_list)\n                else:\n                    df_category = None\n                if df_category is not None:\n                    df.loc[df['there_is_category'], 'taken'] = True\n                    categorized_dfs.append(df_category)\n        processed_df = pd.concat(categorized_dfs)\n        return processed_df\n\n    df_train = normalize_categories(df_train, int(20000*0.8))\n    df_test = normalize_categories(df_test, int(20000*0.2))\n\n    MODEL_PATH = 'allenai/scibert_scivocab_uncased'\n\n    train_dataset = datasets.Dataset.from_pandas(df_train)\n\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, lowercase=True)\n\n    def encode_batch(batch):\n        return tokenizer(\n          batch[\"text\"],\n          max_length=500,\n          truncation=True,\n          padding=\"max_length\"\n        )\n\n    train_dataset = train_dataset.map(encode_batch, batched=True)\n    train_dataset = train_dataset.remove_columns(['text'])\n    print(len(train_dataset))\n\n    train_dataset = train_dataset.remove_columns(['category_number'])\n    train_dataset = train_dataset.remove_columns(['categories'])\n    train_dataset = train_dataset.remove_columns(['taken'])\n    train_dataset = train_dataset.remove_columns(['there_is_category'])\n\n    import os\n    os.mkdir('/kaggle/working/train_dataset')\n    train_dataset.save_to_disk('/kaggle/working/train_dataset')\n\n    MODEL_PATH = 'allenai/scibert_scivocab_uncased'\n\n    test_dataset = datasets.Dataset.from_pandas(df_test)\n\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, lowercase=True)\n\n    def encode_batch(batch):\n        return tokenizer(\n          batch[\"text\"],\n          max_length=500,\n          truncation=True,\n          padding=\"max_length\"\n        )\n\n    test_dataset = test_dataset.map(encode_batch, batched=True)\n\n    test_dataset = test_dataset.remove_columns(['text'])\n    test_dataset = test_dataset.remove_columns(['category_number'])\n    test_dataset = test_dataset.remove_columns(['categories'])\n    test_dataset = test_dataset.remove_columns(['taken'])\n    test_dataset = test_dataset.remove_columns(['there_is_category'])\n\n    os.mkdir('/kaggle/working/test_dataset')\n    print(len(test_dataset))\n    test_dataset.save_to_disk('/kaggle/working/test_dataset')\n\n    import shutil\n    shutil.make_archive('/kaggle/working/train', 'zip', '/kaggle/working/train_dataset')\n    shutil.make_archive('/kaggle/working/test', 'zip', '/kaggle/working/test_dataset')","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:28:28.365696Z","iopub.execute_input":"2023-06-09T12:28:28.366549Z","iopub.status.idle":"2023-06-09T12:28:28.394670Z","shell.execute_reply.started":"2023-06-09T12:28:28.366513Z","shell.execute_reply":"2023-06-09T12:28:28.393042Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"train_dataset = datasets.load_from_disk('/kaggle/input/arxiv-multilable-classification/train')\ntest_dataset = datasets.load_from_disk('/kaggle/input/arxiv-multilable-classification/test')\n\ntest_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\ntrain_dataset.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:28:28.398456Z","iopub.execute_input":"2023-06-09T12:28:28.399196Z","iopub.status.idle":"2023-06-09T12:28:31.536492Z","shell.execute_reply.started":"2023-06-09T12:28:28.399165Z","shell.execute_reply":"2023-06-09T12:28:31.535574Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def scibert_fix(fix_params=True, unfixed_transformers=1):\n    def wrapped_fixator(model):\n        model.classifier.bias = torch.nn.Parameter(torch.ones(model.classifier.bias.shape[0])*(-4))\n        if fix_params:\n            model.requires_grad_(False)\n            if unfixed_transformers > 0:\n                model.classifier.requires_grad_(True)\n                model.dropout.requires_grad_(True)\n                model.bert.pooler.requires_grad_(True)\n                for i in range(unfixed_transformers):\n                    model.bert.encoder.layer[11-i].requires_grad_(True)\n    return wrapped_fixator","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:28:31.539738Z","iopub.execute_input":"2023-06-09T12:28:31.540620Z","iopub.status.idle":"2023-06-09T12:28:31.548051Z","shell.execute_reply.started":"2023-06-09T12:28:31.540586Z","shell.execute_reply":"2023-06-09T12:28:31.547022Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"class WeightedBCE(torch.nn.Module):\n    def __init__(self, false_weight=0.2, weight=None):\n        super(WeightedBCE, self).__init__()\n        self.fw = false_weight\n        self.weight = weight\n\n    def forward(self, input, target):\n        max_val = (-input).clamp(min=0)\n        if self.weight is None:\n            loss = self.fw*(input - input * target) + (self.fw*(1-target)+target)*(max_val + ((-max_val).exp() + (-input - max_val).exp()).log())\n        else:\n            loss = self.fw*(input - input * target) + (self.fw*(1-target)+self.weight*target)*(max_val + ((-max_val).exp() + (-input - max_val).exp()).log())\n        return loss.mean()","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:28:31.551270Z","iopub.execute_input":"2023-06-09T12:28:31.551531Z","iopub.status.idle":"2023-06-09T12:28:31.562829Z","shell.execute_reply.started":"2023-06-09T12:28:31.551507Z","shell.execute_reply":"2023-06-09T12:28:31.561734Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def hamming_score(c, y, threshold=0.5):\n    assert 0 <= threshold <= 1, \"threshold should be between 0 and 1\"\n    p, q = c.shape\n    return 1.0 / (p * q) * (((c > threshold).astype(int) - y) != 0).astype(float).sum()","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:28:31.564829Z","iopub.execute_input":"2023-06-09T12:28:31.565449Z","iopub.status.idle":"2023-06-09T12:28:31.576235Z","shell.execute_reply.started":"2023-06-09T12:28:31.565416Z","shell.execute_reply":"2023-06-09T12:28:31.575200Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"class BertFineTuning(pl.LightningModule):\n    def __init__(self, config):\n        super().__init__()\n        self._config = config\n        self.outputs = []\n\n        if self._config['bert_class'] == transformers.AutoModelForSequenceClassification:\n            self.net = self._config['bert_class'].from_pretrained(self._config['model_path'], num_labels=self._config['num_classes'])\n        else:\n            self.net = self._config['bert_class'](**self._config['net_params'], num_classes=self._config['num_classes'])\n        self.configure_loss()\n        self.configure_net()\n\n        self.valid_truth = []\n        self.valid_preds = []\n\n    def configure_net(self):\n        if self._config['fix_bert'] is not None:\n            self._config['fix_bert'](self.net)\n\n    def configure_loss(self):\n        self.loss = self._config['loss_class'](**self._config['loss_params'])\n\n    def train_dataloader(self):\n        train_dl = DataLoader(self._config['train_dataset'],\n                                                batch_size=self._config['batch_size'],\n                                                shuffle=True,\n                                                num_workers=self._config['num_workers'],\n                                                pin_memory=True)\n        return train_dl\n\n    def val_dataloader(self):\n        val_dl = DataLoader(self._config['val_dataset'],\n                                                batch_size=self._config['batch_size'],\n                                                shuffle=True,\n                                                num_workers=self._config['num_workers'],\n                                                pin_memory=True)\n        return val_dl\n\n    def configure_optimizers(self):\n        opt = self._config['opt_class'](self.net.parameters(), **self._config['opt_params'])\n        scheduler = {\n            'scheduler' : self._config['scheduler_class'](opt, **self._config['scheduler_params']),\n            'interval' : 'step' if self._config['step_every_batch'] else 'epoch'\n        }\n        return {'optimizer':opt, 'lr_scheduler':scheduler}\n\n    def training_step(self, batch, batch_idx):\n        input_ids_ = batch[\"input_ids\"]\n        labels_ = batch['labels']\n        masks_ = batch[\"attention_mask\"]\n    \n        logits = self.net(input_ids = input_ids_, attention_mask = masks_)\n        if self._config['bert_class'] == transformers.AutoModelForSequenceClassification:\n            logits = logits[0]\n        loss = self.loss(logits, labels_.float())\n\n        self.log('loss', loss)\n        return {\"loss\":loss}\n\n    def validation_step(self, batch, batch_idx):\n        input_ids_ = batch[\"input_ids\"]\n        labels_ = batch['labels']\n        masks_ = batch[\"attention_mask\"]\n\n        logits = self.net(input_ids = input_ids_, attention_mask = masks_)\n        if self._config['bert_class'] == transformers.AutoModelForSequenceClassification:\n            logits = logits[0]\n\n        logits_cpu = torch.sigmoid(logits)\n        logits_cpu = logits_cpu.cpu().detach().numpy()\n        labels_cpu = labels_.cpu().numpy()\n        self.valid_truth.extend(labels_cpu)\n        self.valid_preds.extend(logits_cpu)\n        return\n\n    def on_validation_epoch_end(self):\n        self.valid_truth = np.vstack(list(b for b in self.valid_truth))\n        self.valid_preds = np.vstack(list(b for b in self.valid_preds))\n        epoch_tst_micro_roc_auc = roc_auc_score(self.valid_truth, self.valid_preds, average = 'micro')\n        self.last_valid_truth = self.valid_truth\n        self.last_valid_preds = self.valid_preds\n        \n        thresholds = np.arange(0., 1., 0.02)\n        hamming_scores = [self._config['num_classes']*hamming_score(self.valid_preds, self.valid_truth, threshold=threshold) for threshold in thresholds]\n        optim_threshold = np.argmin(hamming_scores)*0.02\n        self.valid_preds = (self.valid_preds>optim_threshold).astype(int)\n        \n        epoch_v_accuracy_score = accuracy_score(self.valid_truth, self.valid_preds)\n        epoch_v_micro_f1_score = f1_score(self.valid_truth, self.valid_preds, average='micro')\n        epoch_hamming_score = hamming_score(self.valid_truth, self.valid_preds, threshold=0.5)*self._config['num_classes']\n\n        self.valid_truth = []\n        self.valid_preds = []\n            \n        self.log(\"accuracy\", epoch_v_accuracy_score)\n        self.log(\"f1\", epoch_v_micro_f1_score)\n        self.log(\"roc_auc\", epoch_tst_micro_roc_auc)\n        self.log(\"hamming_score\", epoch_hamming_score)","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:28:31.577773Z","iopub.execute_input":"2023-06-09T12:28:31.578066Z","iopub.status.idle":"2023-06-09T12:28:31.601963Z","shell.execute_reply.started":"2023-06-09T12:28:31.578034Z","shell.execute_reply":"2023-06-09T12:28:31.601038Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"config = {\n    'bert_class' : transformers.AutoModelForSequenceClassification,\n    'model_path' : \"allenai/scibert_scivocab_uncased\", # used only if 'bert_class' == transformers.AutoModelForSequenceClassification\n    'net_params' : {},\n    'num_classes' : 57,\n    'fix_bert' : scibert_fix(True, 3),\n    'opt_class' : adabelief_pytorch.AdaBelief,\n    'opt_params' : {\n        'lr' : 2e-5,\n        'betas' : (0.9, 0.999),\n        'eps' : 1e-16,\n        'weight_decay' : 1e-3,\n        'weight_decouple' : True,\n        'rectify' : True,\n        'fixed_decay' : False,\n        'amsgrad' : False\n    },\n    'scheduler_class' : torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,\n    'scheduler_params' : {\n        'eta_min' : 1e-5,\n        'T_0' : 2\n    },\n    'step_every_batch' : True,\n    'batch_size' : 16,\n    'num_workers' : 2,\n    'train_dataset' : train_dataset,\n    'val_dataset' : test_dataset,\n    'loss_class' : WeightedBCE,\n    'loss_params' : {'false_weight' : 0.2}, \n    'hamming_threshold' : 0.5\n}","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:28:31.603800Z","iopub.execute_input":"2023-06-09T12:28:31.604192Z","iopub.status.idle":"2023-06-09T12:28:31.616515Z","shell.execute_reply.started":"2023-06-09T12:28:31.604162Z","shell.execute_reply":"2023-06-09T12:28:31.615618Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"module = BertFineTuning(config)","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:23:01.108477Z","iopub.status.idle":"2023-06-09T12:23:01.109239Z","shell.execute_reply.started":"2023-06-09T12:23:01.109012Z","shell.execute_reply":"2023-06-09T12:23:01.109034Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logger = pl.loggers.WandbLogger(project='Research MultilableClassification', name='bert_finetune modified loss, bias_init, deeper finetune')\ntrainer = pl.Trainer(\n    accelerator=\"gpu\",\n    max_epochs=1,\n    logger=logger,\n    log_every_n_steps=20,\n    limit_train_batches=1500*4*2,\n    limit_val_batches=350*4*2,\n    precision=\"16-mixed\"\n)\ntrainer.fit(module)\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:23:01.110843Z","iopub.status.idle":"2023-06-09T12:23:01.111276Z","shell.execute_reply.started":"2023-06-09T12:23:01.111054Z","shell.execute_reply":"2023-06-09T12:23:01.111075Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2023-06-07T08:22:01.238889Z","iopub.execute_input":"2023-06-07T08:22:01.239245Z","iopub.status.idle":"2023-06-07T08:22:14.330605Z","shell.execute_reply.started":"2023-06-07T08:22:01.239217Z","shell.execute_reply":"2023-06-07T08:22:14.329655Z"},"trusted":true},"execution_count":25,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">bert_finetune modified loss, bias_init, deeper finetune</strong> at: <a href='https://wandb.ai/anihilatorgunn/Research%20MultilableClassification/runs/pb45sphi' target=\"_blank\">https://wandb.ai/anihilatorgunn/Research%20MultilableClassification/runs/pb45sphi</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20230607_081659-pb45sphi/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"def process_target(batched_target, q_length):\n    bsize, n_classes = batched_target.shape\n    #batched_target - (batch_size; Num_classes)\n    output_target = torch.zeros(bsize, q_length, n_classes).type_as(batched_target)\n    padding = torch.ones(bsize, q_length, 1).type_as(output_target)\n    output_target = torch.cat((padding, output_target), dim=2)\n\n    non_zero = torch.nonzero(batched_target)\n    prev_batch_index = None\n    for batch_index, class_index in non_zero:\n        if prev_batch_index != batch_index:\n            q_index = 0\n            prev_batch_index = batch_index\n        else:\n            q_index += 1\n        output_target[batch_index][q_index][class_index + 1] = 1\n        output_target[batch_index][q_index][0] = 0\n\n    return output_target\n\n@torch.no_grad()\ndef hungarian_matcher(logits, target):\n    \"\"\"\n    target - (batch_size, num_classes)\n    logits - (batch_size, seq_length, num_classes+1) <- 'NoClass' included\n    \"\"\"\n    # processed_target - (batch_size, seq_length, num_classes+1)\n    shape_to_unfl = logits.shape[:2]\n    predictions = logits.flatten(0, 1).softmax(-1).unflatten(0, shape_to_unfl)\n    processed_target = process_target(target, q_length=predictions.shape[1]).float()\n    \n    # batch_cost - (batch_size, seq_length, seq_length)\n    batched_cost = -torch.matmul(predictions, processed_target.transpose(1, 2))\n    permutation_list = list(linear_sum_assignment(cost)[1] for cost in batched_cost.cpu())\n    batched_permutation = torch.from_numpy(np.vstack([permutation for permutation in permutation_list]))\n    \n    target_permuted = processed_target.gather(1, batched_permutation.unsqueeze(-1).expand(-1, -1, predictions.size(-1)).to(processed_target.device))\n    return target_permuted","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:28:31.621117Z","iopub.execute_input":"2023-06-09T12:28:31.621426Z","iopub.status.idle":"2023-06-09T12:28:31.633544Z","shell.execute_reply.started":"2023-06-09T12:28:31.621403Z","shell.execute_reply":"2023-06-09T12:28:31.632674Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"class HungarianLoss(torch.nn.Module):\n    def __init__(self, weight_for_entropy=None):\n        super(HungarianLoss, self).__init__()\n        self.cross_entropy = nn.CrossEntropyLoss(weight=weight_for_entropy)\n        \n    def forward(self, logits, labels):\n        target_permuted = hungarian_matcher(logits, labels)\n        loss = self.cross_entropy(logits.flatten(0, 1), target_permuted.flatten(0, 1))\n        return loss","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:28:31.635182Z","iopub.execute_input":"2023-06-09T12:28:31.635510Z","iopub.status.idle":"2023-06-09T12:28:31.646317Z","shell.execute_reply.started":"2023-06-09T12:28:31.635480Z","shell.execute_reply":"2023-06-09T12:28:31.645452Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"class TransformerPredictor(torch.nn.Module):\n    def __init__(self, d_model=768, nhead=8, dim_feedforward=3072, dropout=0.1, batch_first=True, q_length=30, num_classes=57, num_layers=8, n_linear=2, classifier_init_policy='neg_biased'):\n        super(TransformerPredictor, self).__init__()\n        self.batch_first = batch_first\n        \n        DecoderLayer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=batch_first, activation='relu')\n        self.decoder = nn.TransformerDecoder(DecoderLayer, num_layers)\n        self.querry = nn.Parameter(torch.rand(q_length, d_model))\n        \n        blocks = []\n        for _ in range(n_linear - 1):\n            blocks.append(nn.Linear(d_model, d_model))\n            blocks.append(nn.ReLU())\n        blocks.append(nn.Linear(d_model, num_classes + 1))\n        self.linear_class = nn.Sequential(*blocks)\n        \n        self._classifier_init_policy(classifier_init_policy, num_classes)\n        \n    def _classifier_init_policy(self, classifier_init_policy, num_classes):\n        if classifier_init_policy == 'unbiased':\n            pass\n        elif classifier_init_policy == 'neg_biased':\n            self.linear_class[-1].bias = torch.nn.Parameter(torch.ones(num_classes+1)*(-3))\n        else:\n            raise ValueError('There is no such classifier_init_policy - {}'.format(classifier_init_policy))\n        \n    def forward(self, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n        if self.batch_first:\n            q = self.querry.unsqueeze(0).repeat(memory.shape[0], 1, 1)\n        else:\n            q = self.querry.unsqueeze(1).repeat(1, memory.shape[1], 1)\n        decoded = self.decoder(q, memory, tgt_mask=tgt_mask, memory_mask=memory_mask, tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n        return self.linear_class(decoded)","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:28:31.647734Z","iopub.execute_input":"2023-06-09T12:28:31.648117Z","iopub.status.idle":"2023-06-09T12:28:31.662814Z","shell.execute_reply.started":"2023-06-09T12:28:31.648064Z","shell.execute_reply":"2023-06-09T12:28:31.661701Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"class BertSetClassifier(torch.nn.Module):\n    def __init__(self, predictor_params={}, bert_fix_policy='top1'):\n        super(BertSetClassifier, self).__init__()\n        self.bert_model = transformers.AutoModel.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n        self._fix_bert(bert_fix_policy)\n        \n        self.decoder = TransformerPredictor(**predictor_params)\n        \n    def _fix_bert(self, bert_fix_policy):\n        if bert_fix_policy == \"complete\":\n            self.bert_model.requires_grad_(False)\n        elif bert_fix_policy == \"top1\":\n            self.bert_model.requires_grad_(False)\n            self.bert_model.pooler.requires_grad_(True)\n            self.bert_model.encoder.layer[11].requires_grad_(True)\n        elif bert_fix_policy == 'unfix':\n            pass\n        else:\n            raise ValueError('There is no such bert_fix_policy - {}'.format(bert_fix_policy))\n            \n    def forward(self, input_ids, attention_mask=None):\n        encoded = self.bert_model(input_ids, attention_mask=attention_mask.float())['last_hidden_state']\n        logits = self.decoder(encoded, memory_key_padding_mask=~(attention_mask.bool()))\n        return logits","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:28:31.664465Z","iopub.execute_input":"2023-06-09T12:28:31.664840Z","iopub.status.idle":"2023-06-09T12:28:31.676420Z","shell.execute_reply.started":"2023-06-09T12:28:31.664774Z","shell.execute_reply":"2023-06-09T12:28:31.675548Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def logits2predictions(logits, num_classes=57):\n    # logits - (batch_size, seq_length, num_classes+1)\n    pred_classes_seq = np.argmax(logits.numpy(), axis=2)\n    B, Q = pred_classes_seq.shape\n    preds = np.zeros((B, num_classes+1))\n    \n    indices = np.arange(B)[:, np.newaxis]\n    preds[indices, pred_classes_seq] = 1\n    preds = preds[:,1:]\n    return preds","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:28:31.679026Z","iopub.execute_input":"2023-06-09T12:28:31.680014Z","iopub.status.idle":"2023-06-09T12:28:31.688848Z","shell.execute_reply.started":"2023-06-09T12:28:31.679962Z","shell.execute_reply":"2023-06-09T12:28:31.687921Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"class BertDec(pl.LightningModule):\n    def __init__(self, config):\n        super().__init__()\n        self._config = config\n        self.outputs = []\n        \n        self.net = config['net_class'](**config['net_params'])\n        self.configure_loss()\n\n        self.valid_truth = []\n        self.valid_preds = []\n\n    def configure_loss(self):\n        weight_to_entropy = self._config['label_weight'].requires_grad_(False)\n        self.loss = HungarianLoss(weight_to_entropy)\n        \n    def train_dataloader(self):\n        train_dl = DataLoader(self._config['train_dataset'],\n                                                batch_size=self._config['batch_size'],\n                                                shuffle=True,\n                                                num_workers=self._config['num_workers'],\n                                                pin_memory=True)\n        return train_dl\n\n    def val_dataloader(self):\n        val_dl = DataLoader(self._config['val_dataset'],\n                                                batch_size=self._config['batch_size'],\n                                                num_workers=self._config['num_workers'],\n                                                pin_memory=True)\n        return val_dl\n\n    def configure_optimizers(self):\n        opt = self._config['opt_class'](self.net.parameters(), **self._config['opt_params'])\n        scheduler = {\n            'scheduler' : self._config['scheduler_class'](opt, **self._config['scheduler_params']),\n            'interval' : 'step' if self._config['step_every_batch'] else 'epoch'\n        }\n\n        return {'optimizer':opt, 'lr_scheduler':scheduler}\n\n    def training_step(self, batch, batch_idx):\n        input_ids = batch[\"input_ids\"]\n        labels = batch['labels']\n        attention_mask = batch[\"attention_mask\"]\n    \n        logits = self.net(input_ids=input_ids, attention_mask=attention_mask)        \n        loss = self.loss(logits, labels.float())\n\n        self.log('loss', loss)\n        return {\"loss\":loss}\n\n    def validation_step(self, batch, batch_idx):\n        input_ids = batch[\"input_ids\"]\n        labels = batch['labels']\n        attention_mask = batch[\"attention_mask\"]\n\n        logits = self.net(input_ids=input_ids, attention_mask=attention_mask)\n        preds = logits2predictions(logits.cpu().detach(), num_classes=self._config['num_classes'])\n\n        labels_cpu = labels.cpu().numpy()\n        self.valid_truth.extend(labels_cpu)\n        self.valid_preds.extend(preds)\n        return\n\n    def on_validation_epoch_end(self):\n        self.valid_truth = np.vstack(list(b for b in self.valid_truth))\n        self.valid_preds = np.vstack(list(b for b in self.valid_preds))\n        epoch_tst_micro_roc_auc = roc_auc_score(self.valid_truth, self.valid_preds, average = 'micro')\n        self.last_valid_truth = self.valid_truth\n        self.last_valid_preds = self.valid_preds\n        \n        epoch_v_accuracy_score = accuracy_score(self.valid_truth, self.valid_preds)\n        epoch_v_micro_f1_score = f1_score(self.valid_truth, self.valid_preds, average='micro')\n        epoch_hamming_score = hamming_score(self.valid_truth, self.valid_preds, threshold=0.5)*self._config['num_classes']\n\n        self.valid_truth = []\n        self.valid_preds = []\n            \n        self.log(\"accuracy\", epoch_v_accuracy_score)\n        self.log(\"f1\", epoch_v_micro_f1_score)\n        self.log(\"roc_auc\", epoch_tst_micro_roc_auc)\n        self.log(\"hamming_score\", epoch_hamming_score)","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:59:35.161782Z","iopub.execute_input":"2023-06-09T12:59:35.162161Z","iopub.status.idle":"2023-06-09T12:59:35.182800Z","shell.execute_reply.started":"2023-06-09T12:59:35.162127Z","shell.execute_reply":"2023-06-09T12:59:35.181729Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"num_labels = 0\ndl = DataLoader(train_dataset, num_workers=2, batch_size=512, drop_last=True)\nwith tqdm(total=len(dl)) as pbar:\n    for batch in dl:\n        pbar.update()\n        num_labels += batch['labels']\nnum_labels = torch.sum(num_labels, dim=0)\nlabel_weight = (1/num_labels) / (1/num_labels).mean()\nlabel_weight = torch.cat((torch.ones(1)*0.1, label_weight))","metadata":{"execution":{"iopub.status.busy":"2023-06-09T16:58:56.583499Z","iopub.execute_input":"2023-06-09T16:58:56.583872Z","iopub.status.idle":"2023-06-09T17:03:30.154844Z","shell.execute_reply.started":"2023-06-09T16:58:56.583841Z","shell.execute_reply":"2023-06-09T17:03:30.153704Z"},"trusted":true},"execution_count":76,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1687 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e5cc594e614e49d096df4208d58d2548"}},"metadata":{}}]},{"cell_type":"code","source":"config_dec = {\n    'net_class' : BertSetClassifier,\n    'net_params' : {'bert_fix_policy':'top1'},\n    'num_classes' : 57,\n    'opt_class' : adabelief_pytorch.AdaBelief,\n    'opt_params' : {\n        'lr' : 2e-5,\n        'betas' : (0.9, 0.999),\n        'eps' : 1e-16,\n        'weight_decay' : 1e-3,\n        'weight_decouple' : True,\n        'rectify' : True,\n        'fixed_decay' : False,\n        'amsgrad' : False\n    },\n    'scheduler_class' : torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,\n    'scheduler_params' : {\n        'eta_min' : 1e-5,\n        'T_0' : 2\n    },\n    'step_every_batch' : True,\n    'batch_size' : 32,\n    'num_workers' : 2,\n    'train_dataset' : train_dataset,\n    'val_dataset' : test_dataset,\n    'apply_alibi' : True,\n    'label_weight' : label_weight\n}","metadata":{"execution":{"iopub.status.busy":"2023-06-09T17:03:46.200630Z","iopub.execute_input":"2023-06-09T17:03:46.201020Z","iopub.status.idle":"2023-06-09T17:03:46.210927Z","shell.execute_reply.started":"2023-06-09T17:03:46.200969Z","shell.execute_reply":"2023-06-09T17:03:46.210035Z"},"trusted":true},"execution_count":78,"outputs":[]},{"cell_type":"code","source":"module = BertDec(config_dec)","metadata":{"execution":{"iopub.status.busy":"2023-06-09T17:03:47.852071Z","iopub.execute_input":"2023-06-09T17:03:47.852441Z","iopub.status.idle":"2023-06-09T17:03:53.889718Z","shell.execute_reply.started":"2023-06-09T17:03:47.852409Z","shell.execute_reply":"2023-06-09T17:03:53.888787Z"},"trusted":true},"execution_count":79,"outputs":[{"name":"stderr","text":"Some weights of the model checkpoint at allenai/scibert_scivocab_uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias']\n- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"}]},{"cell_type":"code","source":"wandb.finish()","metadata":{"execution":{"iopub.status.busy":"2023-06-09T16:35:01.306230Z","iopub.execute_input":"2023-06-09T16:35:01.306606Z","iopub.status.idle":"2023-06-09T16:35:01.311710Z","shell.execute_reply.started":"2023-06-09T16:35:01.306572Z","shell.execute_reply":"2023-06-09T16:35:01.310697Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"logger = pl.loggers.WandbLogger(project='Research MultilableClassification', name='detr-like')\ntrainer = pl.Trainer(\n    accelerator=\"gpu\",\n    max_epochs=2,\n    logger=logger,\n    log_every_n_steps=10,\n    limit_train_batches=1500*4*2,\n    limit_val_batches=350*4*2,\n    precision='16-mixed'\n)\ntrainer.fit(module)\nwandb.finish()","metadata":{"execution":{"iopub.status.busy":"2023-06-09T17:04:04.893406Z","iopub.execute_input":"2023-06-09T17:04:04.893782Z","iopub.status.idle":"2023-06-09T17:04:20.625381Z","shell.execute_reply.started":"2023-06-09T17:04:04.893750Z","shell.execute_reply":"2023-06-09T17:04:20.624268Z"},"trusted":true},"execution_count":80,"outputs":[{"name":"stdout","text":"\u001b[31mPlease check your arguments if you have upgraded adabelief-pytorch from version 0.0.5.\n\u001b[31mModifications to default arguments:\n\u001b[31m                           eps  weight_decouple    rectify\n-----------------------  -----  -----------------  ---------\nadabelief-pytorch=0.0.5  1e-08  False              False\n>=0.1.0 (Current 0.2.0)  1e-16  True               True\n\u001b[34mSGD better than Adam (e.g. CNN for Image Classification)    Adam better than SGD (e.g. Transformer, GAN)\n----------------------------------------------------------  ----------------------------------------------\nRecommended eps = 1e-8                                      Recommended eps = 1e-16\n\u001b[34mFor a complete table of recommended hyperparameters, see\n\u001b[34mhttps://github.com/juntang-zhuang/Adabelief-Optimizer\n\u001b[32mYou can disable the log message by setting \"print_change_log = False\", though it is recommended to keep as a reminder.\n\u001b[0m\nWeight decoupling enabled in AdaBelief\nRectification enabled in AdaBelief\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Sanity Checking: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: 0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d3e6befd6d48475c860b0e74d86837f0"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n","output_type":"stream"}]},{"cell_type":"code","source":"print('Total number of trainable parameters : {}'.format(sum(parameter.numel() for parameter in module.net.parameters() if parameter.requires_grad)))\nprint('Total number of trainable parameters : {}'.format(sum(parameter.numel() for parameter in module.net.decoder.parameters() if parameter.requires_grad)))","metadata":{"execution":{"iopub.status.busy":"2023-06-09T12:20:40.966964Z","iopub.execute_input":"2023-06-09T12:20:40.967511Z","iopub.status.idle":"2023-06-09T12:20:40.989234Z","shell.execute_reply.started":"2023-06-09T12:20:40.967471Z","shell.execute_reply":"2023-06-09T12:20:40.987200Z"},"trusted":true},"execution_count":66,"outputs":[{"name":"stdout","text":"Total number of trainable parameters : 186183226\nTotal number of trainable parameters : 76264762\n","output_type":"stream"},{"name":"stderr","text":"wandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\nwandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\nwandb: ERROR Dropped streaming file chunk (see wandb/debug-internal.log)\n","output_type":"stream"}]},{"cell_type":"code","source":"weight_for_entropy = torch.ones(58, dtype=float).to('cuda')\nweight_for_entropy[0] = 0.1\n\nin_idx_debug = []\nmask_debug = []\n\ndl = DataLoader(train_dataset, batch_size=8)\nnet = module.net.to('cuda')\nLoss = HungarianLoss(weight_for_entropy=weight_for_entropy)\nprocessed_steps = 0\nwith torch.no_grad():\n    for batch in dl:\n        in_idx = batch['input_ids'].to('cuda')\n        labels = batch['labels'].to('cuda')\n        mask = batch['attention_mask'].to('cuda')\n        logits = net(in_idx, mask.bool())\n        processed_steps += 1\n        try:\n            loss = Loss(logits, labels)\n        except:\n            in_idx_debug.append(in_idx)\n            mask_debug.append(mask)\n            break","metadata":{"execution":{"iopub.status.busy":"2023-06-08T16:08:48.483884Z","iopub.execute_input":"2023-06-08T16:08:48.484374Z","iopub.status.idle":"2023-06-08T16:08:49.812222Z","shell.execute_reply.started":"2023-06-08T16:08:48.484335Z","shell.execute_reply":"2023-06-08T16:08:49.811062Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"processed_steps","metadata":{"execution":{"iopub.status.busy":"2023-06-08T16:08:55.573889Z","iopub.execute_input":"2023-06-08T16:08:55.574275Z","iopub.status.idle":"2023-06-08T16:08:55.584502Z","shell.execute_reply.started":"2023-06-08T16:08:55.574246Z","shell.execute_reply":"2023-06-08T16:08:55.582364Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"8"},"metadata":{}}]},{"cell_type":"code","source":"in_idx_debug[0].shape","metadata":{"execution":{"iopub.status.busy":"2023-06-08T16:08:55.701142Z","iopub.execute_input":"2023-06-08T16:08:55.701470Z","iopub.status.idle":"2023-06-08T16:08:55.710874Z","shell.execute_reply.started":"2023-06-08T16:08:55.701444Z","shell.execute_reply":"2023-06-08T16:08:55.709480Z"},"trusted":true},"execution_count":33,"outputs":[{"execution_count":33,"output_type":"execute_result","data":{"text/plain":"torch.Size([8, 500])"},"metadata":{}}]},{"cell_type":"code","source":"mask","metadata":{"execution":{"iopub.status.busy":"2023-06-08T16:07:57.509730Z","iopub.execute_input":"2023-06-08T16:07:57.510092Z","iopub.status.idle":"2023-06-08T16:07:57.523208Z","shell.execute_reply.started":"2023-06-08T16:07:57.510063Z","shell.execute_reply":"2023-06-08T16:07:57.520887Z"},"trusted":true},"execution_count":28,"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"tensor([[1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        ...,\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0],\n        [1, 1, 1,  ..., 0, 0, 0]], device='cuda:0')"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Переделать memory_key_padding_mask - когда float - напрямую добавляется!!!","metadata":{}},{"cell_type":"code","source":"a = train_dataset[0]['attention_mask']\na = ~a.reshape(1, -1).bool()\nkey_padding_mask = F._canonical_mask(\n            mask=a,\n            mask_name=\"key_padding_mask\",\n            other_type=F._none_or_dtype(None),\n            other_name=\"attn_mask\",\n            target_type=float\n        )","metadata":{"execution":{"iopub.status.busy":"2023-06-09T11:15:31.978273Z","iopub.execute_input":"2023-06-09T11:15:31.979481Z","iopub.status.idle":"2023-06-09T11:15:31.988623Z","shell.execute_reply.started":"2023-06-09T11:15:31.979441Z","shell.execute_reply":"2023-06-09T11:15:31.987688Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}