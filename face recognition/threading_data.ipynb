{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"MlEmJ13X6bMv"},"outputs":[],"source":["import numpy as np  # Load required libs\n","import pandas as pd\n","import torch\n","import torchvision\n","from torch.nn.functional import one_hot\n","from threading import Thread, Lock\n","from queue import Empty, Queue\n","import gc\n","import time"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Efexkfbw7bab"},"outputs":[],"source":["def clear_cuda():\n","    gc.collect()\n","    torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yeDd4QNJt_sC"},"outputs":[],"source":["class Queue_with_cleaning(Queue):\n","    def __init__(self, maxsize):\n","        super(Queue_with_cleaning, self).__init__(maxsize)\n","\n","    def clear(self):\n","        while not self.empty():\n","            try:\n","                self.get(block=False)\n","            except Empty:\n","                continue"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bWTpj64B6fDy"},"outputs":[],"source":["class Controller(object):\n","    \"\"\"\n","    Class for control threads and queues\n","    There is no need to create own threads for each task, just modificate their \n","    behavior. So Controller exists. It is always parameter of thread-function,\n","    so it can be called when necessary.\n","\n","\n","    When task starts:\n","    1)Every worker calls controller.get_status() to understand what to DO\n","    2)Every worker calls controller.boil_process() to make controller know how\n","    many workers are working on the task\n","    3)Every worker starts it's own infinity loop with checking if status changed\n","\n","    When task changes:\n","    1)Controller changes it's field __status to 'cleaning'\n","    -> workers when they'll have done their PART of work will STOP and call \n","    controller.freeze_thread() to make controller know how many workers are\n","    still working\n","    2)Controller calls cleaning to free queues(because there are only to queues\n","    to reduce using memory).\n","    3)When there are no active threads(hot_threads == 0) \n","    controller finally changes it's field __status to new status\n","    4)All workers are still in infinity loop, so it's starts with the first \n","    point of 'When task starts'\n","\n","\n","    Once a thread is launched, it should be terminated at some moment.\n","    In case the function of this thread is an infinite loop, one needs a mutex\n","    for signaling a worker thread to break the loop.\n","    The fuction will return, and the thread will be terminated.\n","    \"\"\"\n","    def __init__(self):\n","        self.to_kill = False\n","        self.__status = 'train'\n","        self.lock = Lock()\n","\n","        self.hot_threads = 0\n","\n","    def is_kill(self):\n","        return self.to_kill\n","\n","    def set_tokill(self, tokill):\n","        self.to_kill = tokill\n","\n","    def get_status(self):\n","        return self.__status\n","\n","    def boil_process(self):\n","        with self.lock:\n","            self.hot_threads += 1\n","    \n","    def freeze_thread(self):\n","        with self.lock:\n","            self.hot_threads -= 1\n","\n","    def cleaning(self, ram_queue, cuda_queue):\n","        self.__status = 'cleaning'\n","        \n","        ram_queue.clear()\n","        cuda_queue.clear()\n","        print('Number of hot threads - {}'.format(self.hot_threads))\n","\n","        if self.hot_threads < 0:\n","            raise ValueError('Something went wrong, hot_threads cannot be < 0')\n","        if self.hot_threads > 0:\n","            time.sleep(1)\n","            self.cleaning(ram_queue, cuda_queue)\n","\n","    def change_status(self, new_status, ram_queue, cuda_queue):\n","        self.cleaning(ram_queue, cuda_queue)\n","        self.__status = new_status\n","        time.sleep(1)\n","        print('{} processes have been starter'.format(self.hot_threads))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_iXb7gG-7pDH"},"outputs":[],"source":["def getting_loop(controller, data_generator, queue):\n","    controller.boil_process()\n","    print('process has been started')\n","    for sample_batch in data_generator:\n","        queue.put(sample_batch, block=True)\n","        if controller.get_status() == 'cleaning':\n","            print('threads has been frozen')\n","            controller.freeze_thread()\n","            break\n","        if controller.is_kill():\n","            break\n","\n","def threaded_batches_feeder(controller, train_generator, val_generator, ram_queue):\n","    \"\"\"\n","    Threaded worker for taking data from data-generators and put it in queue\n","    Controlled by controller\n","    \"\"\"\n","    status_generator_correspondence = {'train':train_generator, 'validate':val_generator}\n","\n","    while not controller.is_kill():\n","        status = controller.get_status()\n","\n","        if status == 'cleaning':\n","            time.sleep(1)\n","            continue\n","        else:\n","            current_generator = status_generator_correspondence[status]\n","            getting_loop(controller, current_generator, ram_queue)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jAysjRuz6JzJ"},"outputs":[],"source":["def data_loop(controller, ram_queue, cuda_queue, img_transform, device, data_handler):\n","    controller.boil_process()\n","    print('process has been started')\n","    while not controller.is_kill():\n","        sample_batch = ram_queue.get(block=True)\n","\n","        result_batch = None\n","        with torch.no_grad():\n","            result_batch = data_handler(sample_batch, img_transform=img_transform, device=device)\n","        cuda_queue.put(result_batch, block=True)\n","        \n","        if controller.is_kill():\n","            break\n","        if controller.get_status() == 'cleaning':\n","            print('thread has been frozen')\n","            controller.freeze_thread()\n","            break\n","            \n","def data_handler_celeba(sample_batch, img_transform, device):\n","    result_batch = None\n","    for data in sample_batch:\n","        img, label = data\n","        img = img.to(device)\n","        if img_transform:\n","            img = img_transform(img)\n","        img = img.reshape(1, *img.shape)\n","        label = torch.tensor([label, ]).to(device)\n","        \n","        data_pack = [img, label]\n","        if result_batch is None:\n","            result_batch = data_pack\n","        else:\n","            for idx, data_obj in enumerate(result_batch):\n","                result_batch[idx] = torch.cat((data_obj, data_pack[idx]))\n","    return result_batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DA2ygqEp1MlY"},"outputs":[],"source":["def threaded_cuda_batches(controller,\n","                          ram_queue,\n","                          cuda_queue,\n","                          img_transform_train,\n","                          img_transform_val,\n","                          device,\n","                          handler_train,\n","                          handler_val):\n","    while not controller.is_kill():\n","        status = controller.get_status()\n","\n","        if status == 'cleaning':\n","            time.sleep(1)\n","            continue\n","        else:\n","            data_loop(controller,\n","                      ram_queue,\n","                      cuda_queue,\n","                      img_transform_val if status=='validate' else img_transform_train,\n","                      device,\n","                      handler_val if status=='validate' else handler_train)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j9abM-5hrInn"},"outputs":[],"source":["class Thread_data_processing:\n","    def __init__(self,\n","                 train_generator,\n","                 val_generator,\n","                 device,\n","                 img_transform_train,\n","                 img_transform_val,\n","                 ram_queue_length=12,\n","                 cuda_queue_length=5,\n","                 num_workers_ram=3,\n","                 num_workers_cuda=1,\n","                 handler_train=data_handler_celeba,\n","                 handler_val=data_handler_celeba):\n","        self.device = device\n","        self.handler_train = handler_train\n","        self.handler_val = handler_val\n","        self.train_generator = train_generator\n","        self.val_generator = val_generator\n","        self.img_transform_train = img_transform_train\n","        self.img_transform_val = img_transform_val\n","        self.controller = Controller()\n","\n","        self.ram_queue_length = ram_queue_length\n","        self.cuda_queue_length = cuda_queue_length\n","        self.num_workers_ram = num_workers_ram\n","        self.num_workers_cuda = num_workers_cuda\n","\n","        self.ram_queue = Queue_with_cleaning(maxsize=ram_queue_length)\n","        self.cuda_queue = Queue_with_cleaning(maxsize=cuda_queue_length)\n","\n","    def start(self):\n","        for _ in range(self.num_workers_ram):\n","            thread = Thread(target=threaded_batches_feeder,\n","                            args=(self.controller,\n","                                  self.train_generator,\n","                                  self.val_generator,\n","                                  self.ram_queue))\n","            thread.start()\n","\n","        for _ in range(self.num_workers_cuda):\n","            thread = Thread(target=threaded_cuda_batches,\n","                            args=(self.controller,\n","                                  self.ram_queue,\n","                                  self.cuda_queue,\n","                                  self.img_transform_train,\n","                                  self.img_transform_val,\n","                                  self.device,\n","                                  self.handler_train,\n","                                  self.handler_val))\n","            thread.start()\n","\n","    def change_task(self, new_status):\n","        self.controller.change_status(new_status, self.ram_queue, self.cuda_queue)\n","\n","    def get(self, block=True):\n","        return self.cuda_queue.get(block=True)\n","\n","    def get_train_batch_size(self):\n","        return self.train_generator.get_batch_size()\n","\n","    def get_val_batch_size(self):\n","        return self.val_generator.get_batch_size()\n","\n","    def get_train_steps_per_epoch(self):\n","        return self.train_generator.get_steps_per_epoch()\n","\n","    def get_val_steps_per_epoch(self):\n","        return self.val_generator.get_steps_per_epoch()\n","    \n","    def stop_and_clear(self):\n","        self.controller.set_tokill(True)\n","        for _ in range(self.cuda_queue_length):\n","            try:\n","                self.cuda_queue.get(block=True, timeout=1)\n","            except Empty:\n","                pass\n","        for _ in range(self.ram_queue_length):\n","            try:\n","                self.ram_queue.get(block=True, timeout=1)\n","            except Empty:\n","                pass\n","\n","        with self.ram_queue.mutex:\n","            self.ram_queue.queue.clear()\n","        with self.cuda_queue.mutex:\n","            self.cuda_queue.queue.clear() \n","\n","        clear_cuda()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"accelerator":"GPU","gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}